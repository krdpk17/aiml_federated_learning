{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krdpk17/aiml_federated_learning/blob/main/FLA_Research_Demos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1wMgfaTx1D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35c2242-b5bb-447b-ef70-c17952092769"
      },
      "source": [
        "!pip install --quiet --upgrade tensorflow-federated\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 819 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 65.1 MB 45 kB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 32.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 251 kB 34.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 887 kB 23.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 8.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 949 kB/s \n",
            "\u001b[K     |████████████████████████████████| 121 kB 34.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 38.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 29.1 MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.5.13 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apL4mFdF0mmn"
      },
      "source": [
        "import collections\n",
        "import io\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "# import tensorflow_text as tf_text\n",
        "import tensorflow_federated as tff\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYpKo9Xf5YVE"
      },
      "source": [
        "#Introduction to TensorFlow Federated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf2uxCTX5bqx",
        "outputId": "e15e707b-c7a9-4e7f-c466-57a84ea42c27"
      },
      "source": [
        "# Load simulation data.\n",
        "source, _ = tff.simulation.datasets.emnist.load_data()\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return source.create_tf_dataset_for_client(source.client_ids[n]).map(\n",
        "      lambda e: (tf.reshape(e['pixels'], [-1]), e['label'])\n",
        "  ).repeat(10).batch(20)\n",
        "\n",
        "# Pick a subset of client devices to participate in training.\n",
        "train_data = [client_data(n) for n in range(3)]\n",
        "\n",
        "# Wrap a Keras model for use with TFF.\n",
        "def model_fn() -> tff.learning.Model:\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(10, tf.nn.softmax, input_shape=(784,),\n",
        "                            kernel_initializer='zeros')\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "      model,\n",
        "      input_spec=train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Simulate a few rounds of training with the selected client devices.\n",
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "  model_fn,\n",
        "  client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1))\n",
        "state = trainer.initialize()\n",
        "for _ in range(5):\n",
        "  state, metrics = trainer.next(state, train_data)\n",
        "  print(metrics['train']['loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading emnist_all.sqlite.lzma: 100%|██████████| 170507172/170507172 [00:48<00:00, 3243366.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.897555\n",
            "14.327109\n",
            "14.385421\n",
            "14.352639\n",
            "14.375886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQgEYlhSNThP"
      },
      "source": [
        "# Federated Optimization: Best Practices and Baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB9rMsvDQC8u"
      },
      "source": [
        "# Configuring client pre-processing\n",
        "train_client_spec = tff.simulation.baselines.ClientSpec(\n",
        "    num_epochs=1, batch_size=32)\n",
        "\n",
        "# Creating a baseline task\n",
        "task = tff.simulation.baselines.emnist.create_character_recognition_task(\n",
        "    train_client_spec, model_id='cnn', only_digits=True)\n",
        "\n",
        "print(task.datasets.summary())\n",
        "\n",
        "# Constructing FedAvg\n",
        "fed_avg = tff.learning.build_federated_averaging_process(\n",
        "    task.model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1))\n",
        "\n",
        "# Determining which clients are selected at each round\n",
        "def client_selection_fn(round_num):\n",
        "  return task.datasets.sample_train_clients(num_clients=5)\n",
        "\n",
        "# Logging the output to tensorboard\n",
        "output_dir = tempfile.gettempdir()\n",
        "tensorboard_manager = tff.simulation.TensorBoardManager(output_dir)\n",
        "\n",
        "# Creating a validation function\n",
        "test_data = task.datasets.get_centralized_test_data()\n",
        "eval_process = tff.learning.build_federated_evaluation(task.model_fn)\n",
        "def validation_fn(state, round_num):\n",
        "  return eval_process(state.model, [test_data])\n",
        "\n",
        "# Running the algorithm\n",
        "tff.simulation.run_simulation(\n",
        "    fed_avg,\n",
        "    client_selection_fn,\n",
        "    total_rounds=3,\n",
        "    validation_fn=validation_fn,\n",
        "    metrics_managers=[tensorboard_manager])\n",
        "\n",
        "%tensorboard --logdir=$output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vKYmtIyQDG6"
      },
      "source": [
        "# Personalization & Federated Learning\n",
        "\n",
        "Adapted from the [Federated Reconstruction TensorFlow.org tutorial](https://www.tensorflow.org/federated/tutorials/federated_reconstruction_for_matrix_factorization), which goes into more depth and proposes further exercises."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DicCD0B5QGoB"
      },
      "source": [
        "# Below we define several functions that we'll use later, but their details\n",
        "# aren't as important as their usage in the next cell.\n",
        "def download_movielens_data(dataset_path):\n",
        "  r = requests.get(dataset_path)\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall(path='/tmp')\n",
        "\n",
        "def load_movielens_data(data_directory=\"/tmp\"):\n",
        "  \"\"\"Loads MovieLens ratings from data directory.\"\"\"\n",
        "  ratings_df = pd.read_csv(\n",
        "      os.path.join(data_directory, \"ml-1m\", \"ratings.dat\"),\n",
        "      sep=\"::\",\n",
        "      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], engine=\"python\")\n",
        "  # Map movie and user IDs to [0, vocab_size).\n",
        "  movie_mapping = {\n",
        "      old_movie: new_movie for new_movie, old_movie in enumerate(\n",
        "          ratings_df.MovieID.astype(\"category\").cat.categories)\n",
        "  }\n",
        "  user_mapping = {\n",
        "      old_user: new_user for new_user, old_user in enumerate(\n",
        "          ratings_df.UserID.astype(\"category\").cat.categories)\n",
        "  }\n",
        "  ratings_df.MovieID = ratings_df.MovieID.map(movie_mapping)\n",
        "  ratings_df.UserID = ratings_df.UserID.map(user_mapping)\n",
        "  return ratings_df\n",
        "\n",
        "def create_tf_datasets(ratings_df,\n",
        "                       batch_size=5,\n",
        "                       max_examples_per_user=300,\n",
        "                       max_clients=2000,\n",
        "                       train_fraction=0.8):\n",
        "  \"\"\"Creates train and test TF Datasets containing the ratings for all users.\"\"\"\n",
        "  num_users = len(set(ratings_df.UserID))\n",
        "  # Limit to `max_clients` to speed up data loading.\n",
        "  num_users = min(num_users, max_clients)\n",
        "\n",
        "  def rating_batch_map_fn(rating_batch):\n",
        "    return collections.OrderedDict([\n",
        "        (\"x\", tf.cast(rating_batch[:, 1:2], tf.int64)),\n",
        "        (\"y\", tf.cast(rating_batch[:, 2:3], tf.float32))\n",
        "    ])\n",
        "\n",
        "  tf_datasets = []\n",
        "  for user_id in range(num_users):\n",
        "    user_ratings_df = ratings_df[ratings_df.UserID == user_id]\n",
        "    tf_dataset = tf.data.Dataset.from_tensor_slices(user_ratings_df)\n",
        "\n",
        "    # Define preprocessing operations.\n",
        "    tf_dataset = tf_dataset.take(max_examples_per_user).shuffle(\n",
        "        buffer_size=max_examples_per_user, seed=42).batch(batch_size).map(\n",
        "        rating_batch_map_fn,\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    tf_datasets.append(tf_dataset)\n",
        "\n",
        "  np.random.seed(42)\n",
        "  np.random.shuffle(tf_datasets)\n",
        "  train_idx = int(len(tf_datasets) * train_fraction)\n",
        "  return (tf_datasets[:train_idx], tf_datasets[train_idx:])\n",
        "\n",
        "\n",
        "class UserEmbedding(tf.keras.layers.Layer):\n",
        "  \"\"\"Keras layer representing an embedding for a single user, used below.\"\"\"\n",
        "\n",
        "  def __init__(self, num_latent_factors, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.num_latent_factors = num_latent_factors\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.embedding = self.add_weight(\n",
        "        shape=(1, self.num_latent_factors),\n",
        "        initializer='uniform',\n",
        "        dtype=tf.float32,\n",
        "        name='UserEmbeddingKernel')\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.embedding\n",
        "\n",
        "  def compute_output_shape(self):\n",
        "    return (1, self.num_latent_factors)\n",
        "\n",
        "def get_matrix_factorization_model(\n",
        "    num_items: int = 3706,\n",
        "    num_latent_factors: int = 50) -> tff.learning.reconstruction.Model:\n",
        "  \"\"\"Defines a Keras matrix factorization model.\"\"\"\n",
        "  # Layers with variables will be partitioned into global and local layers.\n",
        "  # We'll pass this to `tff.learning.reconstruction.from_keras_model`.\n",
        "  global_layers = []\n",
        "  local_layers = []\n",
        "\n",
        "  item_input = tf.keras.layers.Input(shape=[1], name='Item')\n",
        "  item_embedding_layer = tf.keras.layers.Embedding(\n",
        "      num_items,\n",
        "      num_latent_factors,\n",
        "      name='ItemEmbedding')\n",
        "  global_layers.append(item_embedding_layer)\n",
        "  flat_item_vec = tf.keras.layers.Flatten(name='FlattenItems')(\n",
        "      item_embedding_layer(item_input))\n",
        "\n",
        "  user_embedding_layer = UserEmbedding(\n",
        "      num_latent_factors,\n",
        "      name='UserEmbedding')\n",
        "  local_layers.append(user_embedding_layer)\n",
        "\n",
        "  # The item_input never gets used by the user embedding layer,\n",
        "  # but this allows the model to directly use the user embedding.\n",
        "  flat_user_vec = user_embedding_layer(item_input)\n",
        "\n",
        "  pred = tf.keras.layers.Dot(\n",
        "      1, normalize=False, name='Dot')([flat_user_vec, flat_item_vec])\n",
        "\n",
        "  input_spec = collections.OrderedDict(\n",
        "      x=tf.TensorSpec(shape=[None, 1], dtype=tf.int64),\n",
        "      y=tf.TensorSpec(shape=[None, 1], dtype=tf.float32))\n",
        "\n",
        "  model = tf.keras.Model(inputs=item_input, outputs=pred)\n",
        "  return tff.learning.reconstruction.from_keras_model(\n",
        "      keras_model=model,\n",
        "      global_layers=global_layers,\n",
        "      local_layers=local_layers,\n",
        "      input_spec=input_spec)\n",
        "\n",
        "class RatingAccuracy(tf.keras.metrics.Mean):\n",
        "  \"\"\"Keras metric computing accuracy of reconstructed ratings.\"\"\"\n",
        "\n",
        "  def __init__(self, name='rating_accuracy', **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    absolute_diffs = tf.abs(y_true - y_pred)\n",
        "    example_accuracies = tf.less_equal(absolute_diffs, 0.5)\n",
        "    super().update_state(example_accuracies, sample_weight=sample_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EfVq52xuwvP",
        "outputId": "e6da7b52-76c5-4200-d582-4060df822391"
      },
      "source": [
        "download_movielens_data('http://files.grouplens.org/datasets/movielens/ml-1m.zip')\n",
        "ratings_df = load_movielens_data()\n",
        "print(ratings_df.head())\n",
        "tf_train_datasets, tf_test_datasets = create_tf_datasets(ratings_df)\n",
        "\n",
        "model_fn = get_matrix_factorization_model\n",
        "loss_fn = lambda: tf.keras.losses.MeanSquaredError()\n",
        "metrics_fn = lambda: [RatingAccuracy()]\n",
        "\n",
        "training_process = tff.learning.reconstruction.build_training_process(\n",
        "    model_fn=model_fn,\n",
        "    loss_fn=loss_fn,\n",
        "    metrics_fn=metrics_fn,\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0),\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.5),\n",
        "    reconstruction_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1))\n",
        "\n",
        "evaluation_computation = tff.learning.reconstruction.build_federated_evaluation(\n",
        "    model_fn,\n",
        "    loss_fn=loss_fn,\n",
        "    metrics_fn=metrics_fn,\n",
        "    reconstruction_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1))\n",
        "\n",
        "state = training_process.initialize()\n",
        "for i in range(10):\n",
        "  federated_train_data = np.random.choice(tf_train_datasets, size=50, replace=False).tolist()\n",
        "  state, metrics = training_process.next(state, federated_train_data)\n",
        "  print(f'Train round {i}:', metrics['train'])\n",
        "\n",
        "eval_metrics = evaluation_computation(state.model, tf_test_datasets)\n",
        "print('Final Eval:', eval_metrics['eval'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   UserID  MovieID  Rating  Timestamp\n",
            "0       0     1104       5  978300760\n",
            "1       0      639       3  978302109\n",
            "2       0      853       3  978301968\n",
            "3       0     3177       4  978300275\n",
            "4       0     2162       5  978824291\n",
            "Train round 0: OrderedDict([('rating_accuracy', 0.0), ('loss', 14.147506)])\n",
            "Train round 1: OrderedDict([('rating_accuracy', 0.0), ('loss', 14.87234)])\n",
            "Train round 2: OrderedDict([('rating_accuracy', 0.0), ('loss', 14.348445)])\n",
            "Train round 3: OrderedDict([('rating_accuracy', 0.00030959753), ('loss', 13.815469)])\n",
            "Train round 4: OrderedDict([('rating_accuracy', 0.009458948), ('loss', 11.988757)])\n",
            "Train round 5: OrderedDict([('rating_accuracy', 0.05315162), ('loss', 7.994862)])\n",
            "Train round 6: OrderedDict([('rating_accuracy', 0.10311671), ('loss', 6.508739)])\n",
            "Train round 7: OrderedDict([('rating_accuracy', 0.17852761), ('loss', 4.184402)])\n",
            "Train round 8: OrderedDict([('rating_accuracy', 0.16158989), ('loss', 4.564255)])\n",
            "Train round 9: OrderedDict([('rating_accuracy', 0.2275224), ('loss', 3.5205145)])\n",
            "Final Eval: OrderedDict([('loss', 3.1710815), ('rating_accuracy', 0.23554668)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QclRS1XgQG57"
      },
      "source": [
        "# Differentially Private Federated Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCHVfDavQJt2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40249bb8-cd68-4f62-ff37-6dbf68fbddc3"
      },
      "source": [
        "# Load simulation data.\n",
        "source, _ = tff.simulation.datasets.emnist.load_data()\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return source.create_tf_dataset_for_client(source.client_ids[n]).map(\n",
        "      lambda e: (tf.reshape(e['pixels'], [-1]), e['label'])\n",
        "  ).repeat(10).batch(20)\n",
        "\n",
        "# Pick a subset of client devices to participate in training.\n",
        "train_data = [client_data(n) for n in range(3)]\n",
        "\n",
        "# Wrap a Keras model for use with TFF.\n",
        "def model_fn() -> tff.learning.Model:\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(10, tf.nn.softmax, input_shape=(784,),\n",
        "                            kernel_initializer='zeros')\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "      model,\n",
        "      input_spec=train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Construct DP model update aggregator.\n",
        "model_update_aggregator = tff.learning.dp_aggregator(\n",
        "  noise_multiplier=1e-3,   # z: Determines privacy epsilon.\n",
        "  clients_per_round=3)     # Aggregator needs number of clients per round.\n",
        "\n",
        "# Build FedAvg process with custom aggregator.\n",
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "  model_fn,\n",
        "  client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1),\n",
        "  model_update_aggregation_factory=model_update_aggregator)\n",
        "\n",
        "# Simulate a few rounds of training with the selected client devices.\n",
        "state = trainer.initialize()\n",
        "for _ in range(5):\n",
        "  state, metrics = trainer.next(state, train_data)\n",
        "  print(metrics['train']['loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.897555\n",
            "13.930094\n",
            "14.069341\n",
            "13.934456\n",
            "13.9971695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U32aERqY2v1O"
      },
      "source": [
        "## DP-FTRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwveYPUW21F1"
      },
      "source": [
        "# Load simulation data.\n",
        "source, _ = tff.simulation.datasets.emnist.load_data()\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return source.create_tf_dataset_for_client(source.client_ids[n]).map(\n",
        "      lambda e: (tf.reshape(e['pixels'], [-1]), e['label'])\n",
        "  ).repeat(10).batch(20)\n",
        "\n",
        "# Pick a subset of client devices to participate in training.\n",
        "train_data = [client_data(n) for n in range(3)]\n",
        "\n",
        "# Wrap a Keras model for use with TFF.\n",
        "def model_fn() -> tff.learning.Model:\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(10, tf.nn.softmax, input_shape=(784,),\n",
        "                            kernel_initializer='zeros')\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "      model,\n",
        "      input_spec=train_data[0].element_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Construct Tree Aggregation aggregator for DP-FTRL.\n",
        "model_weight_specs = tff.framework.type_to_tf_tensor_specs(\n",
        "        tff.learning.framework.weights_type_from_model(model_fn).trainable)\n",
        "model_update_aggregator = tff.aggregators.DifferentiallyPrivateFactory.tree_aggregation(\n",
        "    noise_multiplier=1e-3,\n",
        "    clients_per_round=3,\n",
        "    l2_norm_clip=1.,\n",
        "    record_specs=model_weight_specs)\n",
        "\n",
        "# Build FedAvg process with custom aggregator.\n",
        "trainer = tff.learning.build_federated_averaging_process(\n",
        "  model_fn,\n",
        "  client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.1),\n",
        "  model_update_aggregation_factory=model_update_aggregator)\n",
        "\n",
        "# Simulate a few rounds of training with the selected client devices.\n",
        "state = trainer.initialize()\n",
        "for _ in range(5):\n",
        "  state, metrics = trainer.next(state, train_data)\n",
        "  print(metrics['train']['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kLKZarD2K-X"
      },
      "source": [
        "# Private Heavy Hitters via TensorFlow Federated Analytics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFYU7uuatvRC",
        "outputId": "5ce0e097-6c03-46ff-8b7e-cdf98c4a96a0"
      },
      "source": [
        "# Load the simulation data.\n",
        "source, _ = tff.simulation.datasets.shakespeare.load_data()\n",
        "\n",
        "# Preprocessing funtion to tokenize a line into words.\n",
        "@tf.function\n",
        "def tokenize(ds):\n",
        "  \"\"\"Tokenizes a line into words with alphanum characters.\"\"\"\n",
        "  def extract_strings(example):\n",
        "    return tf.expand_dims(example['snippets'], 0)\n",
        "\n",
        "  def tokenize_line(line):\n",
        "    return tf.data.Dataset.from_tensor_slices(tokenizer.tokenize(line)[0])\n",
        "\n",
        "  def mask_all_symbolic_words(word):\n",
        "    return tf.math.logical_not(\n",
        "        tf_text.wordshape(word, tf_text.WordShape.IS_PUNCT_OR_SYMBOL))\n",
        "\n",
        "  tokenizer = tf_text.WhitespaceTokenizer()\n",
        "  ds = ds.map(extract_strings)\n",
        "  ds = ds.flat_map(tokenize_line)\n",
        "  ds = ds.map(tf_text.case_fold_utf8)\n",
        "  ds = ds.filter(mask_all_symbolic_words)\n",
        "  return ds\n",
        "\n",
        "# Arguments for the PHH computation\n",
        "batch_size = 5\n",
        "max_words_per_user = 8\n",
        "\n",
        "def client_data(n: int) -> tf.data.Dataset:\n",
        "  return tokenize(source.create_tf_dataset_for_client(\n",
        "      source.client_ids[n])).batch(batch_size)\n",
        "\n",
        "# Pick a subset of client devices to participate in the PHH computation.\n",
        "dataset = [client_data(n) for n in range(10)]\n",
        "\n",
        "def run_simulation(one_round_computation: tff.Computation, dataset):\n",
        "  output = one_round_computation(dataset)\n",
        "  heavy_hitters = output.heavy_hitters\n",
        "  heavy_hitters_counts = output.heavy_hitters_counts\n",
        "  heavy_hitters = [word.decode('utf-8', 'ignore') for word in heavy_hitters]\n",
        "\n",
        "  results = {}\n",
        "  for index in range(len(heavy_hitters)):\n",
        "    results[heavy_hitters[index]] = heavy_hitters_counts[index]\n",
        "  return dict(results)\n",
        "\n",
        "iblt_computation = tff.analytics.heavy_hitters.iblt.build_iblt_computation(\n",
        "    capacity=100,\n",
        "    max_string_length=20,\n",
        "    max_words_per_user=max_words_per_user,\n",
        "    max_heavy_hitters=10,\n",
        "    multi_contribution=False,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "result = run_simulation(iblt_computation, dataset)\n",
        "print(f'result without DP: {result}')\n",
        "\n",
        "# See https://github.com/google/differential-privacy/blob/main/common_docs/Delta_For_Thresholding.pdf\n",
        "# for a central differential privacy algorithm on open set histograms.\n",
        "\n",
        "# DP parameters\n",
        "eps = 20\n",
        "delta = 0.01\n",
        "\n",
        "# Calculating scale for Laplace noise\n",
        "scale = max_words_per_user / eps\n",
        "\n",
        "# Calculating the threshold\n",
        "tau = 1 + (max_words_per_user / eps) * np.log(max_words_per_user / (2 * delta))\n",
        "\n",
        "result_with_dp = {}\n",
        "for word in result:\n",
        "  noised_count = result[word] + np.random.laplace(scale=scale)\n",
        "  if noised_count >= tau:\n",
        "    result_with_dp[word] = noised_count\n",
        "print(f'result with DP: {result_with_dp}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading shakespeare.sqlite.lzma: 100%|██████████| 1329828/1329828 [00:00<00:00, 11388110.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result without DP: {'to': 8, 'the': 8, 'and': 7, 'i': 4, 'you': 4, 'a': 3, 'your': 3, 'is': 3, 'he': 3, 'of': 2}\n",
            "result with DP: {'to': 9.212163929285731, 'the': 7.876369565680992, 'and': 6.648564021859482, 'i': 4.693963904208316, 'you': 3.5797036427177726, 'a': 3.442068329594145}\n"
          ]
        }
      ]
    }
  ]
}